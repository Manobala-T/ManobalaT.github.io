{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102966,"databundleVersionId":12412856,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/soil-classification-part-2/soil_competition-2025/train'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:58:00.494142Z","iopub.execute_input":"2025-05-23T09:58:00.495194Z","iopub.status.idle":"2025-05-23T09:58:01.878034Z","shell.execute_reply.started":"2025-05-23T09:58:00.495163Z","shell.execute_reply":"2025-05-23T09:58:01.876903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ“Œ SECTION 1: IMPORT LIBRARIES\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ðŸ“Œ SECTION 2: LOAD DATA (FIXED)\n\ntrain_csv_path = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv'\ntrain_img_dir  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train'\n\ntest_csv_path  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv'\ntest_img_dir   = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test'\n\n# Load and inspect the training data\ntrain_df = pd.read_csv(train_csv_path)\ntrain_df.columns = train_df.columns.str.strip()  # Clean any accidental spaces\n\nprint(\"Columns in train_df:\", train_df.columns)  # âœ… See what the actual column names are\n\n# Set up label mapping\nclass_mapping = {\n    'Alluvial soil': 0,\n    'Black Soil': 1,\n    'Clay soil': 2,\n    'Red soil': 3\n}\n\n# ðŸ›  Check the column name here â€” most likely it is 'soil Type' with space\n# So use the correct name as seen in the print statement above:\ntrain_df['label'] = train_df['soil_type'].map(class_mapping)\n\n# ðŸ“Œ SECTION 3: IMAGE TRANSFORMATIONS\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n# ðŸ“Œ SECTION 4: CUSTOM DATASET CLASS\n\nclass SoilDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transform=None, is_test=False):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        image_id = self.dataframe.iloc[idx, 0]\n        image_path = os.path.join(self.image_dir, image_id)\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        \n        if self.is_test:\n            return image, image_id\n        else:\n            label = self.dataframe.iloc[idx]['label']\n            return image, label\n# ðŸ“Œ SECTION 5: TRAIN/VALIDATION SPLIT\n\ntrain_split, val_split = train_test_split(train_df, test_size=0.2, stratify=train_df['label'])\n\ntrain_dataset = SoilDataset(train_split, train_img_dir, transform)\nval_dataset = SoilDataset(val_split, train_img_dir, transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n# ðŸ“Œ SECTION 6: MODEL SETUP\n\nmodel = models.resnet18(pretrained=True)\nmodel.fc = torch.nn.Linear(model.fc.in_features, 4)  # 4 classes\nmodel = model.to(device)\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n# ðŸ“Œ SECTION 7: TRAINING LOOP\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}\")\n# ðŸ“Œ SECTION 8: VALIDATION\n\nmodel.eval()\ny_true, y_pred = [], []\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images = images.to(device)\n        outputs = model(images)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        y_pred.extend(preds)\n        y_true.extend(labels.numpy())\n\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=list(class_mapping.keys())))\n# ðŸ“Œ SECTION 9: PREDICTION ON TEST SET & SUBMISSION\n\ntest_df = pd.read_csv(test_csv_path)\ntest_dataset = SoilDataset(test_df, test_img_dir, transform, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\nmodel.eval()\npredictions = []\nimage_ids = []\n\nwith torch.no_grad():\n    for images, ids in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        predictions.extend(preds)\n        image_ids.extend(ids)\n\n# Decode predictions\ninverse_class_map = {v: k for k, v in class_mapping.items()}\npredicted_labels = [inverse_class_map[p] for p in predictions]\n\nsubmission_df = pd.DataFrame({'image_id': image_ids, 'soil_type': predicted_labels})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"âœ… submission.csv created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:58:01.879814Z","iopub.execute_input":"2025-05-23T09:58:01.880179Z","iopub.status.idle":"2025-05-23T09:58:01.954944Z","shell.execute_reply.started":"2025-05-23T09:58:01.880149Z","shell.execute_reply":"2025-05-23T09:58:01.953723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ðŸ“Œ SECTION 2: LOAD DATA (FIXED)\n\ntrain_csv_path = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv'\ntrain_img_dir  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train'\n\ntest_csv_path  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv'\ntest_img_dir   = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test'\n\n# Load and inspect the training data\ntrain_df = pd.read_csv(train_csv_path)\ntrain_df.columns = train_df.columns.str.strip()  # Clean any accidental spaces\n\nprint(\"Columns in train_df:\", train_df.columns)  # âœ… See what the actual column names are\n\n# Set up label mapping\nclass_mapping = {\n    'Alluvial soil': 0,\n    'Black Soil': 1,\n    'Clay soil': 2,\n    'Red soil': 3\n}\n\n# ðŸ›  Check the column name here â€” most likely it is 'soil Type' with space\n# So use the correct name as seen in the print statement above:\ntrain_df['label'] = train_df['soil_type'].map(class_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:02:26.734821Z","iopub.execute_input":"2025-05-23T10:02:26.735198Z","iopub.status.idle":"2025-05-23T10:02:26.798275Z","shell.execute_reply.started":"2025-05-23T10:02:26.735174Z","shell.execute_reply":"2025-05-23T10:02:26.796971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_csv_path = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv'\ntrain_img_dir  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train'\n\ntest_csv_path  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv'\ntest_img_dir   = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test'\n\n# Load and inspect the training data\ntrain_df = pd.read_csv(train_csv_path)\ntrain_df.columns = train_df.columns.str.strip()  # Clean any accidental spaces\n\nprint(\"Columns in train_df:\", train_df.columns)  # âœ… See what the actual column names are\n\n# Set up label mapping\nclass_mapping = {\n    'Alluvial soil': 0,\n    'Black Soil': 1,\n    'Clay soil': 2,\n    'Red soil': 3\n}\n\n# ðŸ›  Check the column name here â€” most likely it is 'soil Type' with space\n# So use the correct name as seen in the print statement above:\ntrain_df['label'] = train_df['soil_type'].map(class_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:03:48.207046Z","iopub.execute_input":"2025-05-23T10:03:48.207877Z","iopub.status.idle":"2025-05-23T10:03:48.271956Z","shell.execute_reply.started":"2025-05-23T10:03:48.207852Z","shell.execute_reply":"2025-05-23T10:03:48.270593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a cleaned soil_classification.ipynb notebook file with the provided code\nfrom pathlib import Path\n\nnotebook_code = \"\"\"\n{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"import-libraries\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 1: IMPORT LIBRARIES\\\\n\",\n    \"import os\\\\n\",\n    \"import numpy as np\\\\n\",\n    \"import pandas as pd\\\\n\",\n    \"from PIL import Image\\\\n\",\n    \"import matplotlib.pyplot as plt\\\\n\",\n    \"from sklearn.metrics import classification_report\\\\n\",\n    \"from sklearn.model_selection import train_test_split\\\\n\",\n    \"\\\\n\",\n    \"import torch\\\\n\",\n    \"from torch.utils.data import Dataset, DataLoader\\\\n\",\n    \"from torchvision import transforms, models\\\\n\",\n    \"\\\\n\",\n    \"device = torch.device(\\\\\\\"cuda\\\\\\\" if torch.cuda.is_available() else \\\\\\\"cpu\\\\\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"load-data\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 2: LOAD DATA (FIXED)\\\\n\",\n    \"train_csv_path = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv'\\\\n\",\n    \"train_img_dir  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train'\\\\n\",\n    \"\\\\n\",\n    \"test_csv_path  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv'\\\\n\",\n    \"test_img_dir   = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test'\\\\n\",\n    \"\\\\n\",\n    \"# Load and clean training labels\\\\n\",\n    \"train_df = pd.read_csv(train_csv_path)\\\\n\",\n    \"train_df.columns = train_df.columns.str.strip()\\\\n\",\n    \"\\\\n\",\n    \"print(\\\\\\\"Columns in train_df:\\\\\\\", train_df.columns)\\\\n\",\n    \"\\\\n\",\n    \"# \\\\u2705 Define class mapping\\\\n\",\n    \"class_mapping = {\\\\n\",\n    \"    'Alluvial soil': 0,\\\\n\",\n    \"    'Black Soil': 1,\\\\n\",\n    \"    'Clay soil': 2,\\\\n\",\n    \"    'Red soil': 3\\\\n\",\n    \"}\\\\n\",\n    \"train_df['label'] = train_df['soil Type'].map(class_mapping)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"transformations\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 3: IMAGE TRANSFORMATIONS\\\\n\",\n    \"transform = transforms.Compose([\\\\n\",\n    \"    transforms.Resize((224, 224)),\\\\n\",\n    \"    transforms.RandomHorizontalFlip(),\\\\n\",\n    \"    transforms.RandomRotation(10),\\\\n\",\n    \"    transforms.ToTensor(),\\\\n\",\n    \"    transforms.Normalize(mean=[0.485, 0.456, 0.406],\\\\n\",\n    \"                         std=[0.229, 0.224, 0.225])\\\\n\",\n    \"])\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"dataset-class\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 4: CUSTOM DATASET CLASS\\\\n\",\n    \"class SoilDataset(Dataset):\\\\n\",\n    \"    def __init__(self, dataframe, image_dir, transform=None, is_test=False):\\\\n\",\n    \"        self.dataframe = dataframe\\\\n\",\n    \"        self.image_dir = image_dir\\\\n\",\n    \"        self.transform = transform\\\\n\",\n    \"        self.is_test = is_test\\\\n\",\n    \"\\\\n\",\n    \"    def __len__(self):\\\\n\",\n    \"        return len(self.dataframe)\\\\n\",\n    \"\\\\n\",\n    \"    def __getitem__(self, idx):\\\\n\",\n    \"        image_id = self.dataframe.iloc[idx, 0]\\\\n\",\n    \"        image_path = os.path.join(self.image_dir, image_id)\\\\n\",\n    \"        image = Image.open(image_path).convert(\\\\\\\"RGB\\\\\\\")\\\\n\",\n    \"        if self.transform:\\\\n\",\n    \"            image = self.transform(image)\\\\n\",\n    \"        if self.is_test:\\\\n\",\n    \"            return image, image_id\\\\n\",\n    \"        else:\\\\n\",\n    \"            label = self.dataframe.iloc[idx]['label']\\\\n\",\n    \"            return image, label\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"train-validation-split\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 5: TRAIN/VALIDATION SPLIT\\\\n\",\n    \"train_split, val_split = train_test_split(train_df, test_size=0.2, stratify=train_df['label'])\\\\n\",\n    \"train_dataset = SoilDataset(train_split, train_img_dir, transform)\\\\n\",\n    \"val_dataset = SoilDataset(val_split, train_img_dir, transform)\\\\n\",\n    \"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\\\\n\",\n    \"val_loader = DataLoader(val_dataset, batch_size=32)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"model-setup\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 6: MODEL SETUP\\\\n\",\n    \"model = models.resnet18(pretrained=True)\\\\n\",\n    \"model.fc = torch.nn.Linear(model.fc.in_features, 4)\\\\n\",\n    \"model = model.to(device)\\\\n\",\n    \"criterion = torch.nn.CrossEntropyLoss()\\\\n\",\n    \"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"training-loop\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 7: TRAINING LOOP\\\\n\",\n    \"epochs = 5\\\\n\",\n    \"for epoch in range(epochs):\\\\n\",\n    \"    model.train()\\\\n\",\n    \"    running_loss = 0.0\\\\n\",\n    \"    for images, labels in train_loader:\\\\n\",\n    \"        images, labels = images.to(device), labels.to(device)\\\\n\",\n    \"        optimizer.zero_grad()\\\\n\",\n    \"        outputs = model(images)\\\\n\",\n    \"        loss = criterion(outputs, labels)\\\\n\",\n    \"        loss.backward()\\\\n\",\n    \"        optimizer.step()\\\\n\",\n    \"        running_loss += loss.item()\\\\n\",\n    \"    print(f\\\\\\\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}\\\\\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"validation\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 8: VALIDATION\\\\n\",\n    \"model.eval()\\\\n\",\n    \"y_true, y_pred = [], []\\\\n\",\n    \"with torch.no_grad():\\\\n\",\n    \"    for images, labels in val_loader:\\\\n\",\n    \"        images = images.to(device)\\\\n\",\n    \"        outputs = model(images)\\\\n\",\n    \"        preds = torch.argmax(outputs, dim=1).cpu().numpy()\\\\n\",\n    \"        y_pred.extend(preds)\\\\n\",\n    \"        y_true.extend(labels.numpy())\\\\n\",\n    \"print(\\\\\\\"Classification Report:\\\\\\\")\\\\n\",\n    \"print(classification_report(y_true, y_pred, target_names=list(class_mapping.keys())))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"submission\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\\\U0001F4CC SECTION 9: PREDICTION ON TEST SET & SUBMISSION\\\\n\",\n    \"test_df = pd.read_csv(test_csv_path)\\\\n\",\n    \"test_dataset = SoilDataset(test_df, test_img_dir, transform, is_test=True)\\\\n\",\n    \"test_loader = DataLoader(test_dataset, batch_size=32)\\\\n\",\n    \"model.eval()\\\\n\",\n    \"predictions, image_ids = [], []\\\\n\",\n    \"with torch.no_grad():\\\\n\",\n    \"    for images, ids in test_loader:\\\\n\",\n    \"        images = images.to(device)\\\\n\",\n    \"        outputs = model(images)\\\\n\",\n    \"        preds = torch.argmax(outputs, dim=1).cpu().numpy()\\\\n\",\n    \"        predictions.extend(preds)\\\\n\",\n    \"        image_ids.extend(ids)\\\\n\",\n    \"inverse_class_map = {v: k for k, v in class_mapping.items()}\\\\n\",\n    \"predicted_labels = [inverse_class_map[p] for p in predictions]\\\\n\",\n    \"submission_df = pd.DataFrame({'image_id': image_ids, 'soil_type': predicted_labels})\\\\n\",\n    \"submission_df.to_csv('submission.csv', index=False)\\\\n\",\n    \"print(\\\\\\\"\\\\u2705 submission.csv created successfully.\\\\\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\",\n   \"version\": \"\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n\"\"\"\n\n# Save as .ipynb file\nnotebook_path = Path(\"/mnt/data/soil_classification.ipynb\")\nnotebook_path.write_text(notebook_code)\nnotebook_path.name\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:08:37.942537Z","iopub.execute_input":"2025-05-23T10:08:37.943373Z","iopub.status.idle":"2025-05-23T10:08:37.972160Z","shell.execute_reply.started":"2025-05-23T10:08:37.943344Z","shell.execute_reply":"2025-05-23T10:08:37.970900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": None,\n   \"id\": \"import-libraries\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\U0001F4CC SECTION 1: IMPORT LIBRARIES\\n\",\n    \"import os\\n\",\n    \"import numpy as np\\n\",\n    \"import pandas as pd\\n\",\n    \"from PIL import Image\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"from sklearn.metrics import classification_report\\n\",\n    \"from sklearn.model_selection import train_test_split\\n\",\n    \"\\n\",\n    \"import torch\\n\",\n    \"from torch.utils.data import Dataset, DataLoader\\n\",\n    \"from torchvision import transforms, models\\n\",\n    \"\\n\",\n    \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": None,\n   \"id\": \"load-data\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \\U0001F4CC SECTION 2: LOAD DATA (FIXED)\\n\",\n    \"train_csv_path = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv'\\n\",\n    \"train_img_dir  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train'\\n\",\n    \"\\n\",\n    \"test_csv_path  = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv'\\n\",\n    \"test_img_dir   = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test'\\n\",\n    \"\\n\",\n    \"train_df = pd.read_csv(train_csv_path)\\n\",\n    \"train_df.columns = train_df.columns.str.strip()\\n\",\n    \"print(\\\"Columns in train_df:\\\", train_df.columns)\\n\",\n    \"class_mapping = {\\n\",\n    \"    'Alluvial soil': 0,\\n\",\n    \"    'Black Soil': 1,\\n\",\n    \"    'Clay soil': 2,\\n\",\n    \"    'Red soil': 3\\n\",\n    \"}\\n\",\n    \"train_df['label'] = train_df['soil Type'].map(class_mapping)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:33:57.266504Z","iopub.execute_input":"2025-05-23T10:33:57.266820Z","iopub.status.idle":"2025-05-23T10:33:57.277148Z","shell.execute_reply.started":"2025-05-23T10:33:57.266796Z","shell.execute_reply":"2025-05-23T10:33:57.276247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"import pandas as pd\n\n# Example: replace these with your actual output values\nimage_ids = ['img_001.jpg', 'img_002.jpg', 'img_003.jpg']\npredictions = [0, 2, 1]\n\n# Mapping numeric prediction to soil types\ninverse_class_map = {\n    0: 'Alluvial soil',\n    1: 'Black Soil',\n    2: 'Clay soil',\n    3: 'Red soil'\n}\n\n# Convert predictions to readable soil types\npredicted_labels = [inverse_class_map[p] for p in predictions]\n\n# Create and save DataFrame\nsubmission_df = pd.DataFrame({\n    'image_id': image_ids,\n    'soil_type': predicted_labels\n})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"âœ… submission.csv created successfully.\")\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"import pandas as pd\n\n# Example: replace these with your actual output values\nimage_ids = ['img_001.jpg', 'img_002.jpg', 'img_003.jpg']\npredictions = [0, 2, 1]\n\n# Mapping numeric prediction to soil types\ninverse_class_map = {\n    0: 'Alluvial soil',\n    1: 'Black Soil',\n    2: 'Clay soil',\n    3: 'Red soil'\n}\n\n# Convert predictions to readable soil types\npredicted_labels = [inverse_class_map[p] for p in predictions]\n\n# Create and save DataFrame\nsubmission_df = pd.DataFrame({\n    'image_id': image_ids,\n    'soil_type': predicted_labels\n})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"âœ… submission.csv created successfully.\")\n","metadata":{}},{"cell_type":"code","source":"# âœ… Final prediction step (after model inference)\n\n# Convert predicted class labels to names\ninverse_class_map = {v: k for k, v in class_mapping.items()}\npredicted_labels = [inverse_class_map[p] for p in predictions]\n\n# Create DataFrame\nsubmission_df = pd.DataFrame({\n    'image_id': image_ids,\n    'soil_type': predicted_labels\n})\n\n# Save to CSV\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"âœ… submission.csv created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T10:39:23.704049Z","iopub.execute_input":"2025-05-23T10:39:23.704374Z","iopub.status.idle":"2025-05-23T10:39:23.722559Z","shell.execute_reply.started":"2025-05-23T10:39:23.704353Z","shell.execute_reply":"2025-05-23T10:39:23.721508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Example: replace these with your actual output values\nimage_ids = ['img_001.jpg', 'img_002.jpg', 'img_003.jpg']\npredictions = [0, 2, 1]\n\n# Mapping numeric prediction to soil types\ninverse_class_map = {\n    0: 'Alluvial soil',\n    1: 'Black Soil',\n    2: 'Clay soil',\n    3: 'Red soil'\n}\n\n# Convert predictions to readable soil types\npredicted_labels = [inverse_class_map[p] for p in predictions]\n\n# Create and save DataFrame\nsubmission_df = pd.DataFrame({\n    'image_id': image_ids,\n    'soil_type': predicted_labels\n})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"âœ… submission.csv created successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, f1_score\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.applications import EfficientNetB0\n\n# Set random seed for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# 1. Data Preparation ----------------------------------------------------------\n\n# Define paths\nDATA_DIR = \"/kaggle/input/soil-image-classification-challenge/\"\nTRAIN_DIR = os.path.join(DATA_DIR, \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train\")\nTEST_DIR = os.path.join(DATA_DIR, \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train\")\n\n# Load labels\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_DIR, \"/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv\"))\n\n# Binary classification: Convert to 0 (non-soil) and 1 (soil)\n# Assuming label '0' is non-soil and '1' is soil in your dataset\n# Adjust according to your actual label encoding\n\n# 2. Data Preprocessing -------------------------------------------------------\n\n# Image parameters\nIMG_SIZE = (224, 224)  # Standard size for EfficientNet\nBATCH_SIZE = 32\nCHANNELS = 3\n\n# Create data generators with augmentation for training\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2  # Using 20% for validation\n)\n\n# Generator for validation (no augmentation)\nval_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2\n)\n\n# Create generators\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=TRAIN_DIR,\n    x_col=\"image_id\",\n    y_col=\"soil_type\",\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training'\n)\n\nval_generator = val_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=TRAIN_DIR,\n    x_col=\"image_id\",\n    y_col=\"soil_type\",\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation'\n)\n\n# 3. Model Building -----------------------------------------------------------\n\n# Use transfer learning with EfficientNetB0\nbase_model = EfficientNetB0(\n    input_shape=(IMG_SIZE[0], IMG_SIZE[1], CHANNELS),\n    include_top=False,\n    weights='imagenet'\n)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Build custom head\ninputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], CHANNELS))\nx = base_model(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = models.Model(inputs, outputs)\n\n# Compile the model\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=1e-3),\n    loss='binary_crossentropy',\n    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n)\n\nmodel.summary()\n\n# 4. Model Training -----------------------------------------------------------\n\n# Callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True\n)\n\ncheckpoint = ModelCheckpoint(\n    'best_model.h5',\n    monitor='val_f1_score',\n    save_best_only=True,\n    mode='max'\n)\n\n# Custom callback for F1 score\nclass F1ScoreCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        val_pred = (self.model.predict(val_generator) > 0.5).astype(\"int32\")\n        val_true = val_generator.labels\n        f1 = f1_score(val_true, val_pred)\n        print(f\" - val_f1: {f1:.4f}\")\n        logs['val_f1_score'] = f1\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    epochs=30,\n    validation_data=val_generator,\n    callbacks=[early_stopping, checkpoint, F1ScoreCallback()]\n)\n\n# 5. Model Evaluation ---------------------------------------------------------\n\n# Plot training history\ndef plot_history(history):\n    plt.figure(figsize=(12, 4))\n    \n    # Plot accuracy\n    plt.subplot(1, 3, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n    plt.title('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 3, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Val Loss')\n    plt.title('Loss')\n    plt.legend()\n    \n    # Plot F1 score if available\n    if 'val_f1_score' in history.history:\n        plt.subplot(1, 3, 3)\n        plt.plot(history.history['val_f1_score'], label='Val F1 Score')\n        plt.title('F1 Score')\n        plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_history(history)\n\n# Detailed classification report\nval_pred = (model.predict(val_generator) > 0.5).astype(\"int32\")\nprint(classification_report(val_generator.labels, val_pred))\n\n# 6. Fine-tuning (Optional) --------------------------------------------------\n\n# Unfreeze some layers for fine-tuning\nbase_model.trainable = True\nfor layer in base_model.layers[:100]:\n    layer.trainable = False\n\n# Recompile with lower learning rate\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=1e-5),\n    loss='binary_crossentropy',\n    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n)\n\n# Fine-tune for a few epochs\nhistory_fine = model.fit(\n    train_generator,\n    epochs=10,\n    initial_epoch=history.epoch[-1],\n    validation_data=val_generator,\n    callbacks=[early_stopping, checkpoint, F1ScoreCallback()]\n)\n\n# 7. Generate Predictions for Test Set ----------------------------------------\n\n# Load best model\nmodel = models.load_model('best_model.h5')\n\n# Create test generator (no shuffle, no augmentation)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=TEST_DIR,\n    x_col=\"image_id\",\n    y_col=None,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode=None,\n    shuffle=False\n)\n\n# Generate predictions\ntest_pred = (model.predict(test_generator) > 0.5).astype(\"int32\")\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'image_id': test_df['image_id'],\n    'soil_type': test_pred.flatten()\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:52:46.110902Z","iopub.execute_input":"2025-05-27T07:52:46.111778Z","iopub.status.idle":"2025-05-27T07:52:46.202724Z","shell.execute_reply.started":"2025-05-27T07:52:46.111750Z","shell.execute_reply":"2025-05-27T07:52:46.201561Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'soil_type'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2726706082.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Create generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m train_generator = train_datagen.flow_from_dataframe(\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mdataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_dataframe\u001b[0;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             )\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         return DataFrameIterator(\n\u001b[0m\u001b[1;32m   1209\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;31m# check that inputs match the required class_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         if (\n\u001b[1;32m    753\u001b[0m             \u001b[0mvalidate_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, df, x_col, y_col, weight_col, classes)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;31m# check labels are string if class_mode is binary or sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sparse\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m                 raise TypeError(\n\u001b[1;32m    820\u001b[0m                     \u001b[0;34m'If class_mode=\"{}\", y_col=\"{}\" column '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'soil_type'"],"ename":"KeyError","evalue":"'soil_type'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom sklearn.metrics import f1_score, classification_report\n\n# Set random seeds for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# 1. Data Loading with Robust Checks -------------------------------------------\n\nDATA_DIR = \"/kaggle/input/soil-classification-part-2/soil_competition-2025\"\nTRAIN_DIR = os.path.join(DATA_DIR, \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train\")\nTEST_DIR = os.path.join(DATA_DIR, \"/kaggle/input/soil-classification-part-2/soil_competition-2025/test\")\n\n# Load training labels with column verification\ntry:\n    train_df = pd.read_csv(os.path.join(DATA_DIR, \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv\"))\n    \n    # Check for expected columns - adjust based on your actual CSV\n    if 'label' in train_df.columns:  # Common alternative column name\n        train_df = train_df.rename(columns={'label': 'soil_type'})\n    elif 'target' in train_df.columns:\n        train_df = train_df.rename(columns={'target': 'soil_type'})\n    \n    # Verify we have the required columns\n    assert 'image_id' in train_df.columns, \"CSV missing 'image_id' column\"\n    assert 'soil_type' in train_df.columns, \"CSV missing label column (tried 'soil_type', 'label', 'target')\"\n    \n    print(f\"Successfully loaded {len(train_df)} training samples\")\n    print(\"Sample data:\")\n    print(train_df.head())\n    \nexcept Exception as e:\n    print(f\"Error loading data: {e}\")\n    print(\"Available files in directory:\")\n    print(os.listdir(DATA_DIR))\n    raise\n\n# 2. Data Preparation ---------------------------------------------------------\n\n# Convert labels to binary (0 or 1) if they aren't already\nif set(train_df['soil_type'].unique()) != {0, 1}:\n    print(\"Converting labels to binary format...\")\n    # Simple binary conversion - adjust based on your label encoding\n    train_df['soil_type'] = train_df['soil_type'].apply(lambda x: 1 if x == 'soil' else 0)\n\n# Image parameters\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\n\n# 3. Data Generators with Enhanced Error Handling -----------------------------\n\ndef create_data_generator(df, directory, is_train=True):\n    datagen_args = {\n        'rescale': 1./255,\n        'validation_split': 0.2\n    }\n    \n    if is_train:\n        datagen_args.update({\n            'rotation_range': 20,\n            'width_shift_range': 0.2,\n            'height_shift_range': 0.2,\n            'shear_range': 0.2,\n            'zoom_range': 0.2,\n            'horizontal_flip': True\n        })\n    \n    datagen = ImageDataGenerator(**datagen_args)\n    \n    try:\n        generator = datagen.flow_from_dataframe(\n            dataframe=df,\n            directory=directory,\n            x_col=\"image_id\",\n            y_col=\"soil_type\",\n            target_size=IMG_SIZE,\n            class_mode='binary',\n            batch_size=BATCH_SIZE,\n            subset='training' if is_train else 'validation',\n            validate_filenames=True\n        )\n        \n        # Test loading one batch to verify everything works\n        test_batch = next(generator)\n        print(f\"Successfully created generator with {len(generator)} batches\")\n        print(f\"Batch shape: {test_batch[0].shape}\")\n        \n        return generator\n        \n    except Exception as e:\n        print(f\"Error creating generator: {e}\")\n        print(\"Troubleshooting info:\")\n        print(f\"- Directory exists: {os.path.exists(directory)}\")\n        if len(df) > 0:\n            sample_file = os.path.join(directory, df.iloc[0]['image_id'])\n            print(f\"- Sample file exists: {os.path.exists(sample_file)} (path: {sample_file})\")\n        raise\n\n# Create generators\ntry:\n    train_generator = create_data_generator(train_df, TRAIN_DIR, is_train=True)\n    val_generator = create_data_generator(train_df, TRAIN_DIR, is_train=False)\nexcept Exception as e:\n    print(\"Failed to create data generators\")\n    raise\n\n# 4. Model Building -----------------------------------------------------------\n\ndef build_model():\n    try:\n        # Use EfficientNetB0 with pre-trained weights\n        base_model = EfficientNetB0(\n            input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n            include_top=False,\n            weights='imagenet'\n        )\n        base_model.trainable = False\n\n        # Create custom model head\n        inputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n        x = base_model(inputs, training=False)\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dense(256, activation='relu')(x)\n        x = layers.Dropout(0.5)(x)\n        outputs = layers.Dense(1, activation='sigmoid')(x)\n\n        model = models.Model(inputs, outputs)\n\n        model.compile(\n            optimizer='adam',\n            loss='binary_crossentropy',\n            metrics=['accuracy', \n                    tf.keras.metrics.Precision(name='precision'),\n                    tf.keras.metrics.Recall(name='recall')]\n        )\n\n        print(\"Model successfully built\")\n        model.summary()\n        return model\n\n    except Exception as e:\n        print(f\"Error building model: {e}\")\n        raise\n\nmodel = build_model()\n\n# 5. Training with F1-Score Tracking ------------------------------------------\n\nclass F1ScoreCallback(tf.keras.callbacks.Callback):\n    def __init__(self, val_generator):\n        super().__init__()\n        self.val_generator = val_generator\n    \n    def on_epoch_end(self, epoch, logs=None):\n        # Get all validation data\n        val_true = []\n        val_pred = []\n        \n        for i in range(len(self.val_generator)):\n            x, y = self.val_generator[i]\n            batch_pred = (self.model.predict(x, verbose=0) > 0.5).astype(\"int32\")\n            val_true.extend(y)\n            val_pred.extend(batch_pred.flatten())\n        \n        f1 = f1_score(val_true, val_pred)\n        print(f\" - val_f1: {f1:.4f}\")\n        logs['val_f1'] = f1\n\n# Training callbacks\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\n    F1ScoreCallback(val_generator)\n]\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    epochs=30,\n    validation_data=val_generator,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# 6. Evaluation and Prediction ------------------------------------------------\n\n# Load best model\nmodel = models.load_model('best_model.h5')\n\n# Evaluate on validation set\nval_pred = (model.predict(val_generator) > 0.5).astype(\"int32\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(val_generator.labels, val_pred))\n\n# Generate predictions for test set (if available)\nif os.path.exists(TEST_DIR):\n    test_df = pd.read_csv(os.path.join(DATA_DIR, \"test_labels.csv\"))\n    \n    test_datagen = ImageDataGenerator(rescale=1./255)\n    test_generator = test_datagen.flow_from_dataframe(\n        dataframe=test_df,\n        directory=TEST_DIR,\n        x_col=\"image_id\",\n        y_col=None,\n        target_size=IMG_SIZE,\n        class_mode=None,\n        shuffle=False,\n        batch_size=BATCH_SIZE\n    )\n    \n    test_pred = (model.predict(test_generator) > 0.5).astype(\"int32\")\n    \n    # Create submission file\n    submission = pd.DataFrame({\n        'image_id': test_df['image_id'],\n        'soil_type': test_pred.flatten()\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission file created: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:16:43.440965Z","iopub.execute_input":"2025-05-27T09:16:43.441194Z","iopub.status.idle":"2025-05-27T09:17:06.260353Z","shell.execute_reply.started":"2025-05-27T09:16:43.441173Z","shell.execute_reply":"2025-05-27T09:17:06.259137Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 09:16:48.437504: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748337408.749266      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748337408.835468      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Successfully loaded 1222 training samples\nSample data:\n           image_id  soil_type\n0  img_ed005410.jpg          1\n1  img_0c5ecd2a.jpg          1\n2  img_ed713bb5.jpg          1\n3  img_12c58874.jpg          1\n4  img_eff357af.jpg          1\nConverting labels to binary format...\nError creating generator: If class_mode=\"binary\", y_col=\"soil_type\" column values must be strings.\nTroubleshooting info:\n- Directory exists: True\n- Sample file exists: True (path: /kaggle/input/soil-classification-part-2/soil_competition-2025/train/img_ed005410.jpg)\nFailed to create data generators\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/266041190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# Create generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mval_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/266041190.py\u001b[0m in \u001b[0;36mcreate_data_generator\u001b[0;34m(df, directory, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         generator = datagen.flow_from_dataframe(\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_dataframe\u001b[0;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             )\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         return DataFrameIterator(\n\u001b[0m\u001b[1;32m   1209\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;31m# check that inputs match the required class_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         if (\n\u001b[1;32m    753\u001b[0m             \u001b[0mvalidate_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, df, x_col, y_col, weight_col, classes)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sparse\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m    820\u001b[0m                     \u001b[0;34m'If class_mode=\"{}\", y_col=\"{}\" column '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m                     \u001b[0;34m\"values must be strings.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: If class_mode=\"binary\", y_col=\"soil_type\" column values must be strings."],"ename":"TypeError","evalue":"If class_mode=\"binary\", y_col=\"soil_type\" column values must be strings.","output_type":"error"}],"execution_count":1}]}